{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c39c85d8-d268-4971-a518-9eb5352cce6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading all sensor data files...\n",
      "Error: Could not find a file. Please double-check your filenames in the 'sensordata' folder.\n",
      "[Errno 2] No such file or directory: 'sensordata/sittingaccelerometer.csv'\n",
      "\n",
      "Step 2: Merging and labeling data...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sitting_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m processed_dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m activity \u001b[38;5;129;01min\u001b[39;00m activities:\n\u001b[1;32m---> 33\u001b[0m     acc_df \u001b[38;5;241m=\u001b[39m \u001b[43mdataframes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mactivity\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_acc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     34\u001b[0m     gyro_df \u001b[38;5;241m=\u001b[39m dataframes[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_gyro\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Merge based on the timestamp column\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sitting_acc'"
     ]
    }
   ],
   "source": [
    "# --- 1. SETUP: Import tools and define your files ---\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# This should match your folder and file naming convention\n",
    "data_path = 'sensordata/'\n",
    "activities = ['sitting', 'walking', 'jogging', 'falling']\n",
    "# --------------------\n",
    "\n",
    "# Load all the individual CSV files into a dictionary\n",
    "dataframes = {}\n",
    "print(\"Step 1: Loading all sensor data files...\")\n",
    "try:\n",
    "    for activity in activities:\n",
    "        # Load accelerometer data\n",
    "        acc_filename = f\"{activity}accelerometer.csv\"\n",
    "        dataframes[f\"{activity}_acc\"] = pd.read_csv(os.path.join(data_path, acc_filename))\n",
    "        \n",
    "        # Load gyroscope data\n",
    "        gyro_filename = f\"{activity}gyroscope.csv\"\n",
    "        dataframes[f\"{activity}_gyro\"] = pd.read_csv(os.path.join(data_path, gyro_filename))\n",
    "    print(\"Files loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not find a file. Please double-check your filenames in the 'sensordata' folder.\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# --- 2. MERGE & LABEL: Combine and label the data for each activity ---\n",
    "print(\"\\nStep 2: Merging and labeling data...\")\n",
    "processed_dfs = []\n",
    "for activity in activities:\n",
    "    acc_df = dataframes[f\"{activity}_acc\"]\n",
    "    gyro_df = dataframes[f\"{activity}_gyro\"]\n",
    "    \n",
    "    # Merge based on the timestamp column\n",
    "    merged_df = pd.merge(acc_df, gyro_df, on='seconds_elapsed', suffixes=('_acc', '_gyro'))\n",
    "    \n",
    "    # Label the data with the activity name\n",
    "    merged_df['activity'] = activity\n",
    "    \n",
    "    processed_dfs.append(merged_df)\n",
    "print(\"Data merged and labeled.\")\n",
    "\n",
    "\n",
    "# --- 3. CONCATENATE: Combine all activities into one master DataFrame ---\n",
    "print(\"\\nStep 3: Creating one master dataset...\")\n",
    "master_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "print(\"Master dataset created.\")\n",
    "\n",
    "\n",
    "# --- 4. INSPECT & SAVE: Check our final product and save it ---\n",
    "print(\"\\n--- Final Dataset Information ---\")\n",
    "master_df.info()\n",
    "\n",
    "print(\"\\n--- Count of records for each activity ---\")\n",
    "print(master_df['activity'].value_counts())\n",
    "\n",
    "# Save the final, processed file\n",
    "master_df.to_csv('sensor_data_processed.csv', index=False)\n",
    "print(\"\\n✅ SUCCESS! Your processed sensor data has been saved to 'sensor_data_processed.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9607b5bc-c2d2-4bb8-af60-de9d9a3ceb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Files the Code EXPECTS to Find ---\n",
      "fallingaccelerometer.csv\n",
      "fallinggyroscope.csv\n",
      "joggingaccelerometer.csv\n",
      "jogginggyroscope.csv\n",
      "sittingaccelerometer.csv\n",
      "sittinggyroscope.csv\n",
      "walkingaccelerometer.csv\n",
      "walkinggyroscope.csv\n",
      "\n",
      "-------------------------------------------------\n",
      "--- 2. Files ACTUALLY IN your 'sensordata' folder ---\n",
      "fallingaccelerometer.csv\n",
      "fallinggyroscope.csv\n",
      "joggingGyroscope.csv\n",
      "joggingaccelerometer.csv\n",
      "sittingacceloremeter.csv\n",
      "sittinggyroscope.csv\n",
      "walkingAccelerometer.csv\n",
      "walkingGyroscope.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "data_path = 'sensordata/'\n",
    "activities = ['sitting', 'walking', 'jogging', 'falling']\n",
    "# --------------------\n",
    "\n",
    "print(\"--- 1. Files the Code EXPECTS to Find ---\")\n",
    "expected_files = []\n",
    "for activity in activities:\n",
    "    expected_files.append(f\"{activity}accelerometer.csv\")\n",
    "    expected_files.append(f\"{activity}gyroscope.csv\")\n",
    "\n",
    "for f in sorted(expected_files):\n",
    "    print(f)\n",
    "\n",
    "print(\"\\n-------------------------------------------------\")\n",
    "print(\"--- 2. Files ACTUALLY IN your 'sensordata' folder ---\")\n",
    "try:\n",
    "    actual_files = os.listdir(data_path)\n",
    "    if not actual_files:\n",
    "        print(\"The 'sensordata' folder appears to be empty.\")\n",
    "    else:\n",
    "        for f in sorted(actual_files):\n",
    "            print(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find the directory '{data_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3082f716-5a38-4b4b-9f20-a2ad56b1f6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Detailed File Loading ---\n",
      "✅ SUCCESS: Loaded 'sittingaccelerometer.csv'\n",
      "✅ SUCCESS: Loaded 'sittinggyroscope.csv'\n",
      "✅ SUCCESS: Loaded 'walkingaccelerometer.csv'\n",
      "✅ SUCCESS: Loaded 'walkinggyroscope.csv'\n",
      "✅ SUCCESS: Loaded 'joggingaccelerometer.csv'\n",
      "✅ SUCCESS: Loaded 'jogginggyroscope.csv'\n",
      "✅ SUCCESS: Loaded 'fallingaccelerometer.csv'\n",
      "✅ SUCCESS: Loaded 'fallinggyroscope.csv'\n",
      "\n",
      "--- Loading Check Complete ---\n",
      "All files loaded successfully! You can now run the next cell with the merging code.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. SETUP: Import tools and define your files ---\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "data_path = 'sensordata/'\n",
    "activities = ['sitting', 'walking', 'jogging', 'falling']\n",
    "# --------------------\n",
    "\n",
    "# This new dictionary will hold our data\n",
    "dataframes = {}\n",
    "all_files_loaded = True # This is a flag to track if everything works\n",
    "\n",
    "print(\"--- Starting Detailed File Loading ---\")\n",
    "\n",
    "# This loop will try to load each file and report its status\n",
    "for activity in activities:\n",
    "    # --- Try to load Accelerometer file ---\n",
    "    acc_filename = f\"{activity}accelerometer.csv\"\n",
    "    try:\n",
    "        full_path = os.path.join(data_path, acc_filename)\n",
    "        dataframes[f\"{activity}_acc\"] = pd.read_csv(full_path)\n",
    "        print(f\"✅ SUCCESS: Loaded '{acc_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ FAILED: Could not load '{acc_filename}'. Error: {e}\")\n",
    "        all_files_loaded = False\n",
    "\n",
    "    # --- Try to load Gyroscope file ---\n",
    "    gyro_filename = f\"{activity}gyroscope.csv\"\n",
    "    try:\n",
    "        full_path = os.path.join(data_path, gyro_filename)\n",
    "        dataframes[f\"{activity}_gyro\"] = pd.read_csv(full_path)\n",
    "        print(f\"✅ SUCCESS: Loaded '{gyro_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ FAILED: Could not load '{gyro_filename}'. Error: {e}\")\n",
    "        all_files_loaded = False\n",
    "\n",
    "print(\"\\n--- Loading Check Complete ---\")\n",
    "if all_files_loaded:\n",
    "    print(\"All files loaded successfully! You can now run the next cell with the merging code.\")\n",
    "else:\n",
    "    print(\"One or more files failed to load. Please check the FAILED ❌ message(s) above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ab0546-b224-476f-b200-174c063df461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 of 4: Loading all 8 sensor data files...\n",
      "Files loaded successfully.\n",
      "\n",
      "Step 2 of 4: Merging and labeling data...\n",
      "Data merged and labeled.\n",
      "\n",
      "Step 3 of 4: Creating the master dataset...\n",
      "Master dataset created.\n",
      "\n",
      "Step 4 of 4: Saving the final processed file...\n",
      "\n",
      "----------------------------------------------------\n",
      "✅ SUCCESS! All steps are complete.\n",
      "The file 'sensor_data_processed.csv' has been created.\n",
      "----------------------------------------------------\n",
      "\n",
      "Final Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26450 entries, 0 to 26449\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   time_acc         26450 non-null  int64  \n",
      " 1   seconds_elapsed  26450 non-null  float64\n",
      " 2   z_acc            26450 non-null  float64\n",
      " 3   y_acc            26450 non-null  float64\n",
      " 4   x_acc            26450 non-null  float64\n",
      " 5   time_gyro        26450 non-null  int64  \n",
      " 6   z_gyro           26450 non-null  float64\n",
      " 7   y_gyro           26450 non-null  float64\n",
      " 8   x_gyro           26450 non-null  float64\n",
      " 9   activity         26450 non-null  object \n",
      "dtypes: float64(7), int64(2), object(1)\n",
      "memory usage: 2.0+ MB\n",
      "\n",
      "Activity Counts:\n",
      "activity\n",
      "walking    13746\n",
      "jogging     5734\n",
      "sitting     5599\n",
      "falling     1371\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# FINAL SCRIPT FOR SENSOR DATA PREPARATION (ALL STEPS)\n",
    "# ===================================================================\n",
    "\n",
    "# --- 1. SETUP: Import tools and define your files ---\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"Step 1 of 4: Loading all 8 sensor data files...\")\n",
    "data_path = 'sensordata/'\n",
    "activities = ['sitting', 'walking', 'jogging', 'falling']\n",
    "dataframes = {}\n",
    "\n",
    "try:\n",
    "    for activity in activities:\n",
    "        acc_filename = f\"{activity}accelerometer.csv\"\n",
    "        dataframes[f\"{activity}_acc\"] = pd.read_csv(os.path.join(data_path, acc_filename))\n",
    "        \n",
    "        gyro_filename = f\"{activity}gyroscope.csv\"\n",
    "        dataframes[f\"{activity}_gyro\"] = pd.read_csv(os.path.join(data_path, gyro_filename))\n",
    "    print(\"Files loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: A file was not found. Please double-check your filenames in the 'sensordata' folder.\")\n",
    "    raise e # Stop the script if a file is missing\n",
    "\n",
    "# --- 2. MERGE & LABEL: Combine data for each activity ---\n",
    "print(\"\\nStep 2 of 4: Merging and labeling data...\")\n",
    "processed_dfs = []\n",
    "for activity in activities:\n",
    "    acc_df = dataframes[f\"{activity}_acc\"]\n",
    "    gyro_df = dataframes[f\"{activity}_gyro\"]\n",
    "    \n",
    "    merged_df = pd.merge(acc_df, gyro_df, on='seconds_elapsed', suffixes=('_acc', '_gyro'))\n",
    "    merged_df['activity'] = activity\n",
    "    processed_dfs.append(merged_df)\n",
    "print(\"Data merged and labeled.\")\n",
    "\n",
    "# --- 3. CONCATENATE: Create one master dataset ---\n",
    "print(\"\\nStep 3 of 4: Creating the master dataset...\")\n",
    "master_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "print(\"Master dataset created.\")\n",
    "\n",
    "# --- 4. INSPECT & SAVE: Check and save the final file ---\n",
    "print(\"\\nStep 4 of 4: Saving the final processed file...\")\n",
    "master_df.to_csv('sensor_data_processed.csv', index=False)\n",
    "\n",
    "print(\"\\n----------------------------------------------------\")\n",
    "print(\"✅ SUCCESS! All steps are complete.\")\n",
    "print(\"The file 'sensor_data_processed.csv' has been created.\")\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"\\nFinal Data Info:\")\n",
    "master_df.info()\n",
    "print(\"\\nActivity Counts:\")\n",
    "print(master_df['activity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e36252a-4fa8-49d1-afbb-5173962acce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on 25079 normal data points.\n",
      "\n",
      "Training the anomaly detection model...\n",
      "Model training complete!\n",
      "\n",
      "--- Model Test Results ---\n",
      "Predictions on normal data (should be all 1s): [ 1  1  1  1  1 -1 -1  1 -1  1]\n",
      "Predictions on falling data (should be all -1s): [1 1 1 ... 1 1 1]\n",
      "\n",
      "✅ SUCCESS! Anomaly detection model saved as 'anomaly_model.joblib'\n"
     ]
    }
   ],
   "source": [
    "# --- 1. SETUP: Import necessary tools ---\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import joblib\n",
    "\n",
    "# --- 2. LOAD DATA: Load the processed sensor data you just created ---\n",
    "df = pd.read_csv('sensor_data_processed.csv')\n",
    "\n",
    "# --- 3. PREPARE DATA: Separate normal and abnormal activities ---\n",
    "# The features are the raw sensor readings.\n",
    "features = [\n",
    "    'x_acc', 'y_acc', 'z_acc',\n",
    "    'x_gyro', 'y_gyro', 'z_gyro'\n",
    "]\n",
    "\n",
    "# Create a DataFrame with only the \"normal\" activities for training.\n",
    "normal_df = df[df['activity'] != 'falling']\n",
    "X_train = normal_df[features]\n",
    "\n",
    "# Create a DataFrame with the \"abnormal\" activity for testing.\n",
    "anomaly_df = df[df['activity'] == 'falling']\n",
    "X_test_anomaly = anomaly_df[features]\n",
    "\n",
    "print(f\"Training model on {len(X_train)} normal data points.\")\n",
    "\n",
    "# --- 4. TRAIN THE MODEL: The \"learning\" phase ---\n",
    "print(\"\\nTraining the anomaly detection model...\")\n",
    "model = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "\n",
    "# We only fit the model on the NORMAL data.\n",
    "model.fit(X_train)\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "# --- 5. TEST THE MODEL: See if it can spot the fall ---\n",
    "# The model predicts '1' for normal and '-1' for an anomaly.\n",
    "normal_predictions = model.predict(X_train.sample(10)) \n",
    "anomaly_predictions = model.predict(X_test_anomaly)\n",
    "\n",
    "print(\"\\n--- Model Test Results ---\")\n",
    "print(f\"Predictions on normal data (should be all 1s): {normal_predictions}\")\n",
    "print(f\"Predictions on falling data (should be all -1s): {anomaly_predictions}\")\n",
    "\n",
    "# --- 6. SAVE THE MODEL: Saving the trained \"watchdog\" to a file ---\n",
    "joblib.dump(model, 'anomaly_model.joblib')\n",
    "print(\"\\n✅ SUCCESS! Anomaly detection model saved as 'anomaly_model.joblib'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b47a5c21-5c56-4bd2-8c94-42745c791624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preparation Complete ---\n",
      "Shape of the input data (X): (522, 100, 6)\n",
      "Shape of the labels (y): (522,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy import stats\n",
    "\n",
    "# --- 1. Load the prepared data ---\n",
    "df = pd.read_csv('sensor_data_processed.csv')\n",
    "\n",
    "# --- 2. Encode the activity labels into numbers ---\n",
    "# The model needs numbers, not text (e.g., walking=0, sitting=1)\n",
    "le = LabelEncoder()\n",
    "df['activity_encoded'] = le.fit_transform(df['activity'])\n",
    "\n",
    "# --- 3. Scale the sensor data ---\n",
    "# This brings all sensor values to a similar scale, which helps the model train better.\n",
    "features = ['x_acc', 'y_acc', 'z_acc', 'x_gyro', 'y_gyro', 'z_gyro']\n",
    "scaler = StandardScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "# --- 4. Create the time-series \"windows\" ---\n",
    "TIME_STEPS = 100  # Each \"window\" or \"sentence\" will have 100 timestamps\n",
    "STEP = 50       # We'll slide the window forward by 50 timestamps each time\n",
    "\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "# Create a window for each activity type separately\n",
    "for activity_type in df['activity'].unique():\n",
    "    subset = df[df['activity'] == activity_type]\n",
    "    \n",
    "    for i in range(0, len(subset) - TIME_STEPS, STEP):\n",
    "        # Get the sensor readings for the window\n",
    "        sequence_features = subset[features].iloc[i: i + TIME_STEPS].values\n",
    "        # Get the single label for that window\n",
    "        label = subset['activity_encoded'].iloc[i: i + TIME_STEPS].mode()[0]\n",
    "        \n",
    "        sequences.append(sequence_features)\n",
    "        labels.append(label)\n",
    "\n",
    "# Convert the lists to NumPy arrays, the format TensorFlow expects\n",
    "X = np.array(sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(\"--- Data Preparation Complete ---\")\n",
    "print(f\"Shape of the input data (X): {X.shape}\")\n",
    "print(f\"Shape of the labels (y): {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cfa6475-2832-4c59-9767-14ded40aefde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhumi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,176</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m18,176\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │             \u001b[38;5;34m260\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,436</span> (72.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,436\u001b[0m (72.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,436</span> (72.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,436\u001b[0m (72.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model... This will take a few minutes.\n",
      "Epoch 1/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.2773 - loss: 1.3798 - val_accuracy: 0.4048 - val_loss: 1.3308\n",
      "Epoch 2/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4160 - loss: 1.3348 - val_accuracy: 0.4762 - val_loss: 1.2923\n",
      "Epoch 3/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.4560 - loss: 1.2911 - val_accuracy: 0.5000 - val_loss: 1.2468\n",
      "Epoch 4/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.5040 - loss: 1.2298 - val_accuracy: 0.5000 - val_loss: 1.1779\n",
      "Epoch 5/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5200 - loss: 1.1560 - val_accuracy: 0.5476 - val_loss: 1.0877\n",
      "Epoch 6/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5333 - loss: 1.1099 - val_accuracy: 0.5476 - val_loss: 1.0427\n",
      "Epoch 7/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.5387 - loss: 1.0782 - val_accuracy: 0.5238 - val_loss: 1.0412\n",
      "Epoch 8/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5360 - loss: 1.0767 - val_accuracy: 0.5238 - val_loss: 1.0133\n",
      "Epoch 9/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5547 - loss: 1.0165 - val_accuracy: 0.5238 - val_loss: 0.9847\n",
      "Epoch 10/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5573 - loss: 1.0074 - val_accuracy: 0.5000 - val_loss: 0.9232\n",
      "Epoch 11/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5360 - loss: 0.9639 - val_accuracy: 0.5714 - val_loss: 0.8282\n",
      "Epoch 12/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5413 - loss: 0.8942 - val_accuracy: 0.5952 - val_loss: 0.8388\n",
      "Epoch 13/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5040 - loss: 0.9321 - val_accuracy: 0.4762 - val_loss: 0.8850\n",
      "Epoch 14/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.4880 - loss: 0.9679 - val_accuracy: 0.5238 - val_loss: 0.9671\n",
      "Epoch 15/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5253 - loss: 0.9768 - val_accuracy: 0.5476 - val_loss: 0.9515\n",
      "Epoch 16/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5067 - loss: 0.9480 - val_accuracy: 0.5238 - val_loss: 0.9326\n",
      "Epoch 17/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5360 - loss: 0.9490 - val_accuracy: 0.5238 - val_loss: 0.9094\n",
      "Epoch 18/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5227 - loss: 0.9189 - val_accuracy: 0.5476 - val_loss: 0.8870\n",
      "Epoch 19/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5520 - loss: 0.9079 - val_accuracy: 0.5714 - val_loss: 0.8631\n",
      "Epoch 20/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5680 - loss: 0.8678 - val_accuracy: 0.5952 - val_loss: 0.8448\n",
      "Model training complete!\n",
      "\n",
      "Evaluating model performance on the test data...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4381 - loss: 1.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Test Data: 43.81%\n",
      "\n",
      "✅ SUCCESS! Activity Classifier model saved as 'activity_classifier_model.h5'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# --- 1. SPLIT DATA: Create training and testing sets ---\n",
    "# We use the X and y arrays we created in the last step.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 2. DEFINE THE MODEL ARCHITECTURE ---\n",
    "print(\"Building the LSTM model...\")\n",
    "\n",
    "model = Sequential()\n",
    "# Add the LSTM layer. \n",
    "# input_shape tells the model the shape of our \"windows\": (100 timestamps, 6 sensor features)\n",
    "model.add(LSTM(units=64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "# Add a Dropout layer to prevent the model from memorizing the data too much.\n",
    "model.add(Dropout(0.5))\n",
    "# Add the final output layer. It has 4 units (one for each activity)\n",
    "# and 'softmax' activation to pick the most likely activity.\n",
    "model.add(Dense(units=len(np.unique(y)), activation='softmax'))\n",
    "\n",
    "# --- 3. COMPILE THE MODEL ---\n",
    "# This step prepares the model for training.\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model's architecture\n",
    "model.summary()\n",
    "\n",
    "# --- 4. TRAIN THE MODEL ---\n",
    "print(\"\\nTraining the model... This will take a few minutes.\")\n",
    "# An epoch is one full pass through the entire training dataset.\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1, # Use 10% of training data for validation\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "# --- 5. EVALUATE THE MODEL ---\n",
    "print(\"\\nEvaluating model performance on the test data...\")\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Model Accuracy on Test Data: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# --- 6. SAVE THE MODEL ---\n",
    "model.save('activity_classifier_model.h5')\n",
    "print(\"\\n✅ SUCCESS! Activity Classifier model saved as 'activity_classifier_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b247d6-5d7f-4955-92ec-6b9d62b88c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoder saved!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Assuming your LabelEncoder is named 'le'\n",
    "joblib.dump(le, 'label_encoder.joblib') \n",
    "print(\"Label encoder saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11b9fe55-3b28-4466-964d-b3058a2ba589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not load the audio file. Please double-check the 'audio_file_path'.\n",
      "Details: [Errno 22] Invalid argument: '\"C:\\\\Users\\\\bhumi\\\\Downloads\\\\archive (4)\\\\go\\\\fb7eb481_nohash_0.wav\"'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhumi\\AppData\\Local\\Temp\\ipykernel_15232\\3955615071.py:14: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(audio_file_path)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- YOUR ACTION NEEDED HERE ---\n",
    "# Update this path to an actual .wav file from the dataset you downloaded\n",
    "# Use the Shift + Right-click -> \"Copy as path\" method\n",
    "audio_file_path = r'\"C:\\Users\\bhumi\\Downloads\\archive (4)\\go\\fb7eb481_nohash_0.wav\"'\n",
    "# --------------------------------\n",
    "\n",
    "try:\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file_path)\n",
    "\n",
    "    # Create the spectrogram\n",
    "    D = librosa.stft(y)\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "    # Display the spectrogram\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='hz')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Spectrogram (What the AI \"Sees\")')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: Could not load the audio file. Please double-check the 'audio_file_path'.\")\n",
    "    print(f\"Details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4d9bf-5f95-40ab-8ce7-dbab1c016471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
